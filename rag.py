import os
from dotenv import load_dotenv

from langchain_community.vectorstores.neo4j_vector import Neo4jVector
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import PromptTemplate
import streamlit as st

# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

class ChatPDF:
    vector_store = None
    retriever = None
    chain = None

    def __init__(self):
        # Neo4jæ¥ç¶šæƒ…å ±
        self.NEO4J_URI = os.getenv("NEO4J_URI")
        self.NEO4J_USER = os.getenv("NEO4J_USER")
        self.NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")

        # ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
        self.model = ChatOllama(model="gemma3:4b")
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã®è¨­å®š
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®š (éŸ“å›½èªã¨æ—¥æœ¬èªã®å›ç­”)
        self.prompt = PromptTemplate.from_template("""
        <start_of_turn>user
        Please answer in Korean and Japanese.
        éŸ“å›½èªã¨æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚
        Question: {question}
        Context: {context}
        <end_of_turn>
        <start_of_turn>model
        Here is the answer based on the provided context:
        ---
        Reference Context:
        {context}
        """)

    def ingest(self, pdf_file_path: str):
        # PDFã‚’ãƒ­ãƒ¼ãƒ‰
        st.write(f"ğŸ“„ PDF íŒŒì¼ ë¡œë“œ ì‹œì‘: {pdf_file_path}")
        docs = PyPDFLoader(file_path=pdf_file_path).load()
        st.write(f"âœ… PDF ë¡œë“œ ì™„ë£Œ, ë¬¸ì„œ ìˆ˜: {len(docs)}")

        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        st.write("âœ‚ï¸ í…ìŠ¤íŠ¸ ë¶„í•  ì‹œì‘")
        chunks = self.text_splitter.split_documents(docs)
        st.write(f"âœ… í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ, ì²­í¬ ìˆ˜: {len(chunks)}")

        # åŸ‹ã‚è¾¼ã¿ã®ä½œæˆ
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", task_type="RETRIEVAL_DOCUMENT")

        # ãƒ‡ãƒãƒƒã‚°ç”¨: chunksã®å‹ã¨æœ€åˆã®è¦ç´ ã‚’ç¢ºèª
        print(f"Type of chunks: {type(chunks)}")
        if chunks:
            print(f"Type of first chunk: {type(chunks[0])}")
            print(f"Content of first chunk (if possible): {chunks[0]}")

        # ãƒ†ã‚­ã‚¹ãƒˆã¨åŸ‹ã‚è¾¼ã¿ã‚’ä¿å­˜
        text_embeddings = [(chunk.page_content, embeddings.embed_query(chunk.page_content)) for chunk in chunks]
        metadatas = [{'reference_text': chunk.page_content} for chunk in chunks]

        # Neo4jã«ä¿å­˜
        self.vector_store = Neo4jVector.from_embeddings(
            text_embeddings=text_embeddings,
            embedding=embeddings,
            metadatas=metadatas,
            url=self.NEO4J_URI,
            username=self.NEO4J_USER,
            password=self.NEO4J_PASSWORD
        )
        st.write("âœ… Neo4j ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ ì™„ë£Œ")

        # ãƒªãƒˆãƒªãƒãƒ¼ã®è¨­å®š
        self.retriever = self.vector_store.as_retriever()
        st.write("âœ… ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì™„ë£Œ")

        # ãƒãƒ£ãƒƒãƒˆãƒã‚§ãƒ¼ãƒ³è¨­å®š
        self.chain = ({"context": self.retriever, "question": RunnablePassthrough()}
                      | self.prompt
                      | self.model
                      | StrOutputParser())
        st.write("âœ… ì±— ì²´ì¸ ìƒì„± ì™„ë£Œ")

    def ask(self, query: str):
        if not self.chain:
            return "ë¨¼ì € PDF íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”."
        return self.chain.invoke(query)

    def clear(self):
        self.vector_store = None
        self.retriever = None
        self.chain = None

